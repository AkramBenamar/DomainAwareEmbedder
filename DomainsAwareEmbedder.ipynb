{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXo9Wt3UK5Tn1vcFx5t131",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkramBenamar/DomainAwareEmbedder/blob/master/DomainsAwareEmbedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DomainsAwareEmbeder"
      ],
      "metadata": {
        "id": "RTNhO5mLwicL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "4K77qXJTwrao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedder"
      ],
      "metadata": {
        "id": "DPr1X5sDw_Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PositionalEncoding"
      ],
      "metadata": {
        "id": "qnq6iHrYzEhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkc-sBQVUZCI",
        "outputId": "d9660a8c-5154-4f34-e0f4-82575a4d672c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.192s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import unittest\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 512) -> None:\n",
        "        super().__init__()\n",
        "        self.pe = self._generate_encoding(d_model, max_len)\n",
        "\n",
        "    def _generate_encoding(self, d_model: int, max_len: int) -> torch.Tensor:\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, seq_len: int) -> torch.Tensor:\n",
        "        return self.pe[:, :seq_len]\n",
        "\n",
        "class TestPositionalEncoding(unittest.TestCase):\n",
        "\n",
        "    def test_shape(self):\n",
        "        \"\"\"Test output shape\"\"\"\n",
        "        d_model = 16\n",
        "        max_len = 100\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output = pe(50)\n",
        "        self.assertEqual(output.shape, (1, 50, d_model))\n",
        "\n",
        "    def test_values_repeatability(self):\n",
        "        \"\"\"Test same output for same inputs\"\"\"\n",
        "        d_model = 32\n",
        "        max_len = 60\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output1 = pe(10)\n",
        "        output2 = pe(10)\n",
        "        self.assertTrue(torch.allclose(output1, output2, atol=1e-6))\n",
        "\n",
        "    def test_no_nan(self):\n",
        "        \"\"\"Test qnot NaN\"\"\"\n",
        "        pe = PositionalEncoding(64, 128)\n",
        "        output = pe(64)\n",
        "        self.assertFalse(torch.isnan(output).any())\n",
        "\n",
        "    def test_known_value(self):\n",
        "        \"\"\"Test values\"\"\"\n",
        "        d_model = 4\n",
        "        max_len = 1\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output = pe(1)[0, 0]  # shape: (d_model,)\n",
        "        expected = torch.tensor([\n",
        "            math.sin(0 / (10000 ** (0 / d_model))),  # sin(0) = 0\n",
        "            math.cos(0 / (10000 ** (0 / d_model))),  # cos(0) = 1\n",
        "            math.sin(0 / (10000 ** (2 / d_model))),  # sin(0) = 0\n",
        "            math.cos(0 / (10000 ** (2 / d_model)))   # cos(0) = 1\n",
        "        ])\n",
        "        self.assertTrue(torch.allclose(output, expected, atol=1e-5))\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPositionalEncoding))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DomainEmbedder"
      ],
      "metadata": {
        "id": "c2CcV66P4oU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "class DomainAwareEmbedder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_domains: int,\n",
        "        d_model: int,\n",
        "        d_embed: int,\n",
        "        n_heads: int = 4,\n",
        "        max_seq_len: int = 512\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.domain_proj_layer = nn.Linear(num_domains, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.query_proj = nn.Linear(d_embed, d_model)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def project_domains(self, m: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Project modality descriptors to embedding space.\"\"\"\n",
        "        return self.domain_proj_layer(m.float())\n",
        "\n",
        "    def combine_domain_and_position(self, domain_proj: torch.Tensor, seq_len: int, device=None) -> torch.Tensor:\n",
        "        \"\"\"Add positional encoding to projected modality embeddings.\"\"\"\n",
        "        pos_enc = self.pos_encoder(seq_len).to(device or domain_proj.device)\n",
        "        return domain_proj + pos_enc\n",
        "\n",
        "    def forward(self, x: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, d_embed)\n",
        "            m: (batch_size, seq_len, num_modalities)\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        domain_proj = self.project_domains(m)\n",
        "        DC = self.combine_domain_and_position(domain_proj, seq_len)\n",
        "\n",
        "        query = self.query_proj(x)\n",
        "        attended, _ = self.attention(query, DC, DC)\n",
        "\n",
        "        return self.layer_norm(attended)\n",
        "\n",
        "\n",
        "import unittest\n",
        "import torch\n",
        "\n",
        "class TestDomainAwareEmbedder(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.batch_size = 2\n",
        "        self.seq_len = 10\n",
        "        self.d_embed = 32\n",
        "        self.d_model = 64\n",
        "        self.num_domains = 5\n",
        "\n",
        "        self.model = DomainAwareEmbedder(\n",
        "            num_domains=self.num_domains,\n",
        "            d_model=self.d_model,\n",
        "            d_embed=self.d_embed,\n",
        "            n_heads=4,\n",
        "            max_seq_len=100\n",
        "        )\n",
        "\n",
        "        self.x = torch.randn(self.batch_size, self.seq_len, self.d_embed)\n",
        "        self.m = torch.randn(self.batch_size, self.seq_len, self.num_domains)\n",
        "\n",
        "    def test_project_domains(self):\n",
        "        projected = self.model.project_domains(self.m)\n",
        "        self.assertEqual(projected.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "        self.assertFalse(torch.isnan(projected).any())\n",
        "\n",
        "    def test_combine_domain_and_position(self):\n",
        "        domain_proj = self.model.project_domains(self.m)\n",
        "        combined = self.model.combine_domain_and_position(domain_proj, self.seq_len)\n",
        "        self.assertEqual(combined.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "        self.assertFalse(torch.isnan(combined).any())\n",
        "\n",
        "    def test_forward_output_shape(self):\n",
        "        output = self.model(self.x, self.m)\n",
        "        self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "\n",
        "    def test_forward_repeatability(self):\n",
        "        output1 = self.model(self.x, self.m)\n",
        "        output2 = self.model(self.x, self.m)\n",
        "        self.assertTrue(torch.allclose(output1, output2, atol=1e-5))\n",
        "\n",
        "    def test_forward_no_nan(self):\n",
        "        output = self.model(self.x, self.m)\n",
        "        self.assertFalse(torch.isnan(output).any())\n",
        "\n",
        "    def test_combine_domain_and_position_adds_encoding(self):\n",
        "        domain_proj = self.model.project_domains(self.m)\n",
        "        combined = self.model.combine_domain_and_position(domain_proj, self.seq_len)\n",
        "        pos_enc = self.model.pos_encoder(self.seq_len).to(domain_proj.device)\n",
        "        diff = combined - domain_proj\n",
        "        self.assertTrue(torch.allclose(diff, pos_enc.expand_as(domain_proj), atol=1e-6))\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestDomainAwareEmbedder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTshFxgU2vrz",
        "outputId": "2b71cf8d-7736-4094-8686-cae8396990e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "......\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.022s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=6 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder"
      ],
      "metadata": {
        "id": "zZ1QZTnZxCiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TransformerEncoder"
      ],
      "metadata": {
        "id": "teAiP6IL6iN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import unittest\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Transformer-based encoder\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dropout: float = 0.1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, hidden_dim), last token representation\n",
        "        \"\"\"\n",
        "        x = self.input_proj(x)\n",
        "        out = self.transformer_encoder(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TestTransformerEncoder(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.batch_size = 4\n",
        "        self.seq_len = 10\n",
        "        self.input_dim = 32\n",
        "        self.hidden_dim = 64\n",
        "        self.encoder = TransformerEncoder(\n",
        "            input_dim=self.input_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            n_heads=4,\n",
        "            n_layers=2\n",
        "        )\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        out = self.encoder(x)\n",
        "        self.assertEqual(out.shape, (self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def test_projection_works(self):\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        projected = self.encoder.input_proj(x)\n",
        "        self.assertEqual(projected.shape, (self.batch_size, self.seq_len, self.hidden_dim))\n",
        "\n",
        "    def test_determinism(self):\n",
        "        torch.manual_seed(42)\n",
        "        self.encoder.eval()\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        out1 = self.encoder(x)\n",
        "        torch.manual_seed(42)\n",
        "        out2 = self.encoder(x)\n",
        "        self.assertTrue(torch.allclose(out1, out2, atol=1e-6))\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestTransformerEncoder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfvx8SQD6p8w",
        "outputId": "8b9483e3-13bd-4f05-e769-2ac41aab08e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.042s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DomainAwareTransformerEncoder"
      ],
      "metadata": {
        "id": "UdiUvId391Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "YNQKaqIpBHYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import unittest\n",
        "import pandas as pd\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "class DomainAwareTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder integrates domain-aware embeddings with a TransformerEncoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_domains: int,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        domain_embed_dim: int,\n",
        "        max_seq_len: int = 512,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dropout: float = 0.1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.domain_embedder = DomainAwareEmbedder(\n",
        "            num_domains=num_domains,\n",
        "            d_model=hidden_dim,\n",
        "            d_embed=input_dim,\n",
        "            n_heads=n_heads,\n",
        "            max_seq_len=max_seq_len\n",
        "        )\n",
        "\n",
        "        self.encoder = TransformerEncoder(\n",
        "            input_dim=hidden_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            n_heads=n_heads,\n",
        "            n_layers=n_layers,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input features of shape (batch_size, seq_len, input_dim)\n",
        "            m: Domain descriptors of shape (batch_size, seq_len, num_domains)\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, hidden_dim), last token representation\n",
        "        \"\"\"\n",
        "        domain_context = self.domain_embedder(x, m)  # (B, S, hidden_dim)\n",
        "        x_proj = self.input_proj(x)  # (B, S, hidden_dim)\n",
        "        enriched = x_proj + domain_context  # Inject domain bias\n",
        "        return self.encoder(enriched)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TestDomainAwareTransformerEncoder(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.batch_size = 2\n",
        "        self.seq_len = 10\n",
        "        self.input_dim = 32\n",
        "        self.hidden_dim = 64\n",
        "        self.domain_embed_dim = 32\n",
        "        self.num_domains = 5\n",
        "\n",
        "        self.model = DomainAwareTransformerEncoder(\n",
        "            num_domains=self.num_domains,\n",
        "            input_dim=self.input_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            domain_embed_dim=self.domain_embed_dim,\n",
        "            max_seq_len=50\n",
        "        )\n",
        "\n",
        "        self.x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        self.m = torch.randn(self.batch_size, self.seq_len, self.num_domains)\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        out = self.model(self.x, self.m)\n",
        "        self.assertEqual(out.shape, (self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def test_embedder_parameters_are_trainable(self):\n",
        "        embedder_params = list(self.model.domain_embedder.parameters())\n",
        "        self.assertTrue(any(p.requires_grad for p in embedder_params))\n",
        "        self.assertTrue(any(p.numel() > 0 for p in embedder_params))\n",
        "\n",
        "    def test_deterministic_output(self):\n",
        "        torch.manual_seed(42)\n",
        "        out1 = self.model(self.x, self.m)\n",
        "        torch.manual_seed(42)\n",
        "        out2 = self.model(self.x, self.m)\n",
        "        self.assertTrue(torch.allclose(out1, out2, atol=1e-5))\n",
        "\n",
        "    def test_total_trainable_params(self):\n",
        "        total_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        self.assertGreater(total_params, 0)\n",
        "\n",
        "    def test_visualize_parameters(self):\n",
        "        model = DomainAwareTransformerEncoder(\n",
        "        num_domains=5,\n",
        "        input_dim=32,\n",
        "        hidden_dim=64,\n",
        "        domain_embed_dim=32,\n",
        "        max_seq_len=50\n",
        "        )\n",
        "\n",
        "\n",
        "        batch_size = 2\n",
        "        seq_len = 10\n",
        "        input_dim = 32\n",
        "        num_domains = 5\n",
        "\n",
        "        summary(\n",
        "            model,\n",
        "            input_data=(torch.randn(batch_size, seq_len, input_dim),\n",
        "                        torch.randn(batch_size, seq_len, num_domains)),\n",
        "            col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "            depth=4,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestDomainAwareTransformerEncoder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcK3Xzq197JS",
        "outputId": "af69ad3c-4317-45e4-e7ed-6b3b56b2dc2a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.071s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================================================================================================\n",
            "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Trainable\n",
            "======================================================================================================================================================\n",
            "DomainAwareTransformerEncoder                      [2, 10, 32]               [2, 64]                   --                        True\n",
            "├─DomainAwareEmbedder: 1-1                         [2, 10, 32]               [2, 10, 64]               --                        True\n",
            "│    └─Linear: 2-1                                 [2, 10, 5]                [2, 10, 64]               384                       True\n",
            "│    └─PositionalEncoding: 2-2                     --                        [1, 10, 64]               --                        --\n",
            "│    └─Linear: 2-3                                 [2, 10, 32]               [2, 10, 64]               2,112                     True\n",
            "│    └─MultiheadAttention: 2-4                     [2, 10, 64]               [2, 10, 64]               16,640                    True\n",
            "│    └─LayerNorm: 2-5                              [2, 10, 64]               [2, 10, 64]               128                       True\n",
            "├─Linear: 1-2                                      [2, 10, 32]               [2, 10, 64]               2,112                     True\n",
            "├─TransformerEncoder: 1-3                          [2, 10, 64]               [2, 64]                   --                        True\n",
            "│    └─Linear: 2-6                                 [2, 10, 64]               [2, 10, 64]               4,160                     True\n",
            "│    └─TransformerEncoder: 2-7                     [2, 10, 64]               [2, 10, 64]               --                        True\n",
            "│    │    └─ModuleList: 3-1                        --                        --                        --                        True\n",
            "│    │    │    └─TransformerEncoderLayer: 4-1      [2, 10, 64]               [2, 10, 64]               49,984                    True\n",
            "│    │    │    └─TransformerEncoderLayer: 4-2      [2, 10, 64]               [2, 10, 64]               49,984                    True\n",
            "======================================================================================================================================================\n",
            "Total params: 125,504\n",
            "Trainable params: 125,504\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 0.15\n",
            "======================================================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.19\n",
            "Params size (MB): 0.30\n",
            "Estimated Total Size (MB): 0.50\n",
            "======================================================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "Kjm7QvNyw5Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNj5UP1fHXR4",
        "outputId": "9656ac17-774d-48c3-e125-1e5ca72f8703"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DataViz"
      ],
      "metadata": {
        "id": "wDyw_vwEDCBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class MultilingualDatasetAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyze a multilingual dataset containing reviews, languages, and product categories.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame, text_column: str = \"review_body\",\n",
        "                 lang_column: str = \"language\", category_column: str = \"product_category\") -> None:\n",
        "\n",
        "        self.df = df.copy()\n",
        "        self.text_column = text_column\n",
        "        self.lang_column = lang_column\n",
        "        self.category_column = category_column\n",
        "\n",
        "        self._validate_columns()\n",
        "\n",
        "    def _validate_columns(self) -> None:\n",
        "        \"\"\"Ensure required columns are present in the dataset.\"\"\"\n",
        "        required = {self.text_column, self.lang_column, self.category_column}\n",
        "        missing = required - set(self.df.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "    def compute_language_distribution(self) -> pd.DataFrame:\n",
        "        \"\"\"Compute count and percentage of reviews per language.\"\"\"\n",
        "        lang_stats = self.df[self.lang_column].value_counts().reset_index()\n",
        "        lang_stats.columns = [\"Language\", \"Count\"]\n",
        "        lang_stats[\"Percentage\"] = 100 * lang_stats[\"Count\"] / len(self.df)\n",
        "        return lang_stats\n",
        "\n",
        "    def unique_categories_per_language(self) -> pd.DataFrame:\n",
        "        \"\"\"List unique product categories for each language.\"\"\"\n",
        "        return (\n",
        "            self.df.groupby(self.lang_column)[self.category_column]\n",
        "            .apply(lambda x: sorted(x.dropna().unique().tolist()))\n",
        "            .reset_index(name=\"Unique Categories\")\n",
        "        )\n",
        "\n",
        "    def sample_review_per_language(self, random_state: int = 42) -> pd.DataFrame:\n",
        "        \"\"\"Sample one example review per language.\"\"\"\n",
        "        return (\n",
        "            self.df.groupby(self.lang_column)[self.text_column]\n",
        "            .apply(lambda x: x.dropna().sample(1, random_state=random_state).values[0]\n",
        "                   if not x.dropna().empty else \"N/A\")\n",
        "            .reset_index(name=\"Example Review\")\n",
        "        )\n",
        "\n",
        "    def plot_language_distribution(self) -> None:\n",
        "        \"\"\"Visualize the number of reviews per language.\"\"\"\n",
        "        stats = self.compute_language_distribution()\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(data=stats, x=\"Language\", y=\"Count\", palette=\"mako\")\n",
        "        plt.title(\"Number of Samples per Language\")\n",
        "        plt.ylabel(\"Review Count\")\n",
        "        plt.xlabel(\"Language\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def summarize(self) -> None:\n",
        "        \"\"\"Print summary statistics and samples.\"\"\"\n",
        "        print(\"Dataset Summary\")\n",
        "        print(f\"Total samples: {len(self.df)}\")\n",
        "        print(f\"Languages ({self.df[self.lang_column].nunique()}): {sorted(self.df[self.lang_column].unique())}\")\n",
        "\n",
        "        print(\"Language Distribution\")\n",
        "        print(self.compute_language_distribution().to_string(index=False))\n",
        "\n",
        "        print(\"Unique Categories per Language\")\n",
        "        print(self.unique_categories_per_language().to_string(index=False))\n",
        "\n",
        "        print(\"Example Review per Language\")\n",
        "        print(self.sample_review_per_language().to_string(index=False))\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Efrei_Datasets/train.csv\")\n",
        "analyzer = MultilingualDatasetAnalyzer(df, text_column=\"review_body\",\n",
        "                                        lang_column=\"language\",\n",
        "                                        category_column=\"product_category\")\n",
        "analyzer.summarize()\n",
        "# analyzer.plot_language_distribution()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhPmb99GIb1L",
        "outputId": "8bb04d98-c299-4aea-8647-2f2926f15c43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Summary\n",
            "Total samples: 1200000\n",
            "Languages (6): ['de', 'en', 'es', 'fr', 'ja', 'zh']\n",
            "Language Distribution\n",
            "Language  Count  Percentage\n",
            "      de 200000   16.666667\n",
            "      en 200000   16.666667\n",
            "      es 200000   16.666667\n",
            "      fr 200000   16.666667\n",
            "      ja 200000   16.666667\n",
            "      zh 200000   16.666667\n",
            "Unique Categories per Language\n",
            "language                                                                                                                                                                                                                                                                                                                                                                        Unique Categories\n",
            "      de [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, digital_video_download, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, personal_care_appliances, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "      en [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, digital_video_download, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, personal_care_appliances, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "      es                         [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, personal_care_appliances, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "      fr                         [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, personal_care_appliances, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "      ja                           [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, digital_video_download, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "      zh                         [apparel, automotive, baby_product, beauty, book, camera, digital_ebook_purchase, drugstore, electronics, furniture, grocery, home, home_improvement, industrial_supplies, jewelry, kitchen, lawn_and_garden, luggage, musical_instruments, office_product, other, pc, personal_care_appliances, pet_products, shoes, sports, toy, video_games, watch, wireless]\n",
            "Example Review per Language\n",
            "language                                                                                                                        Example Review\n",
            "      de                                                                                              Ist ok ...blondierung quillt schnell auf\n",
            "      en                                                                   Not strong enough to run a small 120v vacuum cleaner, to clean car.\n",
            "      es                                                 Mini usb cable de carga defectuoso por lo cual se estropearon los usb de las baterias\n",
            "      fr attention je pensais que c'était de vrais sequins. ce n'est pas le cas mais les enfants seront ravies tout de même car bien brillant.\n",
            "      ja             ハロウィン用に購入しました。ピンがついたふかふかの耳でかわいいけれど、つけた時に安定感がなく難しい。付属のカチューシャにピンを差し込んで併用してもどこか明後日を向いてしまう感じ。使いづらいけれど、かわいいので何とか工夫して使っていきたいです。\n",
            "      zh                                                                            物流，配货非常不满意，速度超慢，希望有所改进。以前在亚马逊买过东西，那时物流比现在给力，不知道为什么现在还越来越差了\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "AWDBkwGhHOyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Trainer"
      ],
      "metadata": {
        "id": "R2efEfD7C-6u"
      }
    }
  ]
}