{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/LapAV6weIEzJcg9vpWB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkramBenamar/DomainAwareEmbedder/blob/master/DomainsAwareEmbedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DomainsAwareEmbeder"
      ],
      "metadata": {
        "id": "RTNhO5mLwicL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "4K77qXJTwrao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedder"
      ],
      "metadata": {
        "id": "DPr1X5sDw_Ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PositionalEncoding"
      ],
      "metadata": {
        "id": "qnq6iHrYzEhb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkc-sBQVUZCI",
        "outputId": "d9660a8c-5154-4f34-e0f4-82575a4d672c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.192s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import unittest\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 512) -> None:\n",
        "        super().__init__()\n",
        "        self.pe = self._generate_encoding(d_model, max_len)\n",
        "\n",
        "    def _generate_encoding(self, d_model: int, max_len: int) -> torch.Tensor:\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  # (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, seq_len: int) -> torch.Tensor:\n",
        "        return self.pe[:, :seq_len]\n",
        "\n",
        "class TestPositionalEncoding(unittest.TestCase):\n",
        "\n",
        "    def test_shape(self):\n",
        "        \"\"\"Test output shape\"\"\"\n",
        "        d_model = 16\n",
        "        max_len = 100\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output = pe(50)\n",
        "        self.assertEqual(output.shape, (1, 50, d_model))\n",
        "\n",
        "    def test_values_repeatability(self):\n",
        "        \"\"\"Test same output for same inputs\"\"\"\n",
        "        d_model = 32\n",
        "        max_len = 60\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output1 = pe(10)\n",
        "        output2 = pe(10)\n",
        "        self.assertTrue(torch.allclose(output1, output2, atol=1e-6))\n",
        "\n",
        "    def test_no_nan(self):\n",
        "        \"\"\"Test qnot NaN\"\"\"\n",
        "        pe = PositionalEncoding(64, 128)\n",
        "        output = pe(64)\n",
        "        self.assertFalse(torch.isnan(output).any())\n",
        "\n",
        "    def test_known_value(self):\n",
        "        \"\"\"Test values\"\"\"\n",
        "        d_model = 4\n",
        "        max_len = 1\n",
        "        pe = PositionalEncoding(d_model, max_len)\n",
        "        output = pe(1)[0, 0]  # shape: (d_model,)\n",
        "        expected = torch.tensor([\n",
        "            math.sin(0 / (10000 ** (0 / d_model))),  # sin(0) = 0\n",
        "            math.cos(0 / (10000 ** (0 / d_model))),  # cos(0) = 1\n",
        "            math.sin(0 / (10000 ** (2 / d_model))),  # sin(0) = 0\n",
        "            math.cos(0 / (10000 ** (2 / d_model)))   # cos(0) = 1\n",
        "        ])\n",
        "        self.assertTrue(torch.allclose(output, expected, atol=1e-5))\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPositionalEncoding))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DomainEmbedder"
      ],
      "metadata": {
        "id": "c2CcV66P4oU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import torch\n",
        "class DomainAwareEmbedder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_domains: int,\n",
        "        d_model: int,\n",
        "        d_embed: int,\n",
        "        n_heads: int = 4,\n",
        "        max_seq_len: int = 512\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.domain_proj_layer = nn.Linear(num_domains, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.query_proj = nn.Linear(d_embed, d_model)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, batch_first=True)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def project_domains(self, m: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Project modality descriptors to embedding space.\"\"\"\n",
        "        return self.domain_proj_layer(m.float())\n",
        "\n",
        "    def combine_domain_and_position(self, domain_proj: torch.Tensor, seq_len: int, device=None) -> torch.Tensor:\n",
        "        \"\"\"Add positional encoding to projected modality embeddings.\"\"\"\n",
        "        pos_enc = self.pos_encoder(seq_len).to(device or domain_proj.device)\n",
        "        return domain_proj + pos_enc\n",
        "\n",
        "    def forward(self, x: torch.Tensor, m: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch_size, seq_len, d_embed)\n",
        "            m: (batch_size, seq_len, num_modalities)\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        domain_proj = self.project_domains(m)\n",
        "        DC = self.combine_domain_and_position(domain_proj, seq_len)\n",
        "\n",
        "        query = self.query_proj(x)\n",
        "        attended, _ = self.attention(query, DC, DC)\n",
        "\n",
        "        return self.layer_norm(attended)\n",
        "\n",
        "\n",
        "import unittest\n",
        "import torch\n",
        "\n",
        "class TestDomainAwareEmbedder(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.batch_size = 2\n",
        "        self.seq_len = 10\n",
        "        self.d_embed = 32\n",
        "        self.d_model = 64\n",
        "        self.num_domains = 5\n",
        "\n",
        "        self.model = DomainAwareEmbedder(\n",
        "            num_domains=self.num_domains,\n",
        "            d_model=self.d_model,\n",
        "            d_embed=self.d_embed,\n",
        "            n_heads=4,\n",
        "            max_seq_len=100\n",
        "        )\n",
        "\n",
        "        self.x = torch.randn(self.batch_size, self.seq_len, self.d_embed)\n",
        "        self.m = torch.randn(self.batch_size, self.seq_len, self.num_domains)\n",
        "\n",
        "    def test_project_domains(self):\n",
        "        projected = self.model.project_domains(self.m)\n",
        "        self.assertEqual(projected.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "        self.assertFalse(torch.isnan(projected).any())\n",
        "\n",
        "    def test_combine_domain_and_position(self):\n",
        "        domain_proj = self.model.project_domains(self.m)\n",
        "        combined = self.model.combine_domain_and_position(domain_proj, self.seq_len)\n",
        "        self.assertEqual(combined.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "        self.assertFalse(torch.isnan(combined).any())\n",
        "\n",
        "    def test_forward_output_shape(self):\n",
        "        output = self.model(self.x, self.m)\n",
        "        self.assertEqual(output.shape, (self.batch_size, self.seq_len, self.d_model))\n",
        "\n",
        "    def test_forward_repeatability(self):\n",
        "        output1 = self.model(self.x, self.m)\n",
        "        output2 = self.model(self.x, self.m)\n",
        "        self.assertTrue(torch.allclose(output1, output2, atol=1e-5))\n",
        "\n",
        "    def test_forward_no_nan(self):\n",
        "        output = self.model(self.x, self.m)\n",
        "        self.assertFalse(torch.isnan(output).any())\n",
        "\n",
        "def test_combine_domain_and_position_adds_encoding(self):\n",
        "    domain_proj = self.model.project_domains(self.m)\n",
        "    combined = self.model.combine_domain_and_position(domain_proj, self.seq_len)\n",
        "    pos_enc = self.model.pos_encoder(self.seq_len).to(domain_proj.device)\n",
        "    diff = combined - domain_proj\n",
        "    self.assertTrue(torch.allclose(diff, pos_enc.expand_as(domain_proj), atol=1e-6))\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestDomainAwareEmbedder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTshFxgU2vrz",
        "outputId": "da1f8a62-1bf1-4c74-ff15-1128b68f802a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.022s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=5 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder"
      ],
      "metadata": {
        "id": "zZ1QZTnZxCiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####TransformerEncoder"
      ],
      "metadata": {
        "id": "teAiP6IL6iN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import unittest\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Transformer-based encoder that replaces RNN/GRU for sequence modeling.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dim: int,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        dropout: float = 0.1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, seq_len, input_dim)\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, hidden_dim), last token representation\n",
        "        \"\"\"\n",
        "        x = self.input_proj(x)\n",
        "        out = self.transformer_encoder(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TestTransformerEncoder(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.batch_size = 4\n",
        "        self.seq_len = 10\n",
        "        self.input_dim = 32\n",
        "        self.hidden_dim = 64\n",
        "        self.encoder = TransformerEncoder(\n",
        "            input_dim=self.input_dim,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            n_heads=4,\n",
        "            n_layers=2\n",
        "        )\n",
        "\n",
        "    def test_output_shape(self):\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        out = self.encoder(x)\n",
        "        self.assertEqual(out.shape, (self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def test_projection_works(self):\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        projected = self.encoder.input_proj(x)\n",
        "        self.assertEqual(projected.shape, (self.batch_size, self.seq_len, self.hidden_dim))\n",
        "\n",
        "    def test_determinism(self):\n",
        "        torch.manual_seed(42)\n",
        "        self.encoder.eval()\n",
        "        x = torch.randn(self.batch_size, self.seq_len, self.input_dim)\n",
        "        out1 = self.encoder(x)\n",
        "        torch.manual_seed(42)\n",
        "        out2 = self.encoder(x)\n",
        "        self.assertTrue(torch.allclose(out1, out2, atol=1e-6))\n",
        "\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestTransformerEncoder))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfvx8SQD6p8w",
        "outputId": "8b9483e3-13bd-4f05-e769-2ac41aab08e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.042s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "Kjm7QvNyw5Vd"
      }
    }
  ]
}